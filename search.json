[{"title":"Introduction to Post Training Quantization","path":"/2025/03/24/250324Introduction-to-Post-Training-Quantization/","content":"1 什么是 Model Quantization (模型量化)？为什么要这样做？Quantization 是一种在保证模型的结构不发生变化的条件下，通过降低模型参数的精度 (bitrate) 或模型 forward 过程中的中间值的精度，从而加速模型的推理效率，减少推理过程中的显存开销的方法。 现在的 SOTA 可以基本做到 2 bits 的低损失量化，这可以使得模型 decoding 效率加速 3 倍，这是一个惊人的表现。 上图为 QTIP 2 bits 量化在 Llama 3-405B 模型上的推理示例。 2 Quantization 领域的研究分支和进展 (Updated: March 24th, 2025)主要的 Quantization 方法分为 Quantization-aware Training 和 Post-training Quantization, 前者是在反向传播更新 weights 的基础上去做量化，耗时相对更高，对算力的要求也更高；而后者是用 Backward-Free 的方法去做，不通过梯度来更新 weights, 相对更省时省力，但是效果往往相对 QAT 会相对差一点点 (但是现在两种方案的 SOTA 其实差不太多，所以 intuitively speaking, training 其实并不是特别关键)。 具体细分的分支，以及 literature review 可以参考这篇文章：A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms","categories":["Efficient Machine Learning"]},{"title":"Hello Cornell","path":"/2025/03/23/250323hello-cornell/","content":"Ithaca 是一个神奇的小村子。Ecstasy 在这里第一次对科研有非功利驱使的热情。 Ecstasy 还能走到远处的山顶吗？没有人知道。 但 Ecstasy 想试试。","categories":["Casual Notes"]}]